#Linformer with RoBERTa-Base trained for ~23 epochs on WikiCorpus, projected_dims: 256, shared-kv

### Input
train_input:
  data_processor: BertMlmOnlyTfRecordsDynamicMaskProcessor
  data_dir: "./wikicorpus/train_cased_msl512_mlm_only_unmasked"
  vocab_file: '../../vocab/google_research_uncased_L-12_H-768_A-12.txt'
  do_lower: False
  max_sequence_length: 512
  max_predictions_per_seq: 80
  mask_whole_word: False
  shuffle: True
  batch_size: 1024
  shuffle_seed: 1203
  steps: 30000

eval_input:
  data_processor: BertMlmOnlyTfRecordsDynamicMaskProcessor
  data_dir: "./wikicorpus/train_cased_msl512_mlm_only_unmasked"
  vocab_file: '../../vocab/google_research_uncased_L-12_H-768_A-12.txt'
  do_lower: False
  max_sequence_length: 512
  max_predictions_per_seq: 80
  mask_whole_word: False
  shuffle: True
  batch_size: 1024
  shuffle_seed: 1204
  steps: 1000

model:
  disable_nsp: True
  # Embedding
  hidden_size: 768
  use_position_embedding: True
  use_segment_embedding: False
  position_embedding_type: "learned" # {"learned", "fixed"}
  max_position_embeddings: 512
  share_embedding_weights: True

  # Encoder
  num_hidden_layers: 12
  dropout_rate: 0.1
  layer_norm_epsilon: 1e-5

  # Encoder - Attention
  num_heads: 12
  attention_type: "scaled_dot_product" # {"dot_product", "scaled_dot_product"}
  attention_dropout_rate: 0.1
  use_projection_bias_in_attention: True
  use_ffn_bias_in_attention: True

  # LinFormer specific
  projected_dims: 256
  attention_style: "linformer-shared-kv" # {"linformer-shared-heads", "linformer-shared-kv"}
  disable_attention: False 
  # If disable_attention=True, replaces the attention computation i.e softmax(QK^T)V and all that follows 
  # with a elementwise multiply of Q and Q
  # Note that the initial projection of K, V matrices specific to Linformer will be computed.
  # This flag is only for DTG debugging purposes.

  # Encoder - ffn
  filter_size: 3072
  encoder_nonlinearity: "gelu"
  use_ffn_bias: True

  # Task-specific
  use_ffn_bias_in_mlm: True
  use_output_bias_in_mlm: True
  mlm_nonlinearity: "gelu"
  mlm_loss_scaling: "precomputed_num_masked" # {"num_masked", "batch_size", "precomputed_num_masked"}
  mlm_loss_weight: 1 

  # use_nsp_bias: True   # add learned bias to NSP layer output
  # nsp_nonlinearity: "tanh"
  # pooler_type: "first" # specifies reduction type of encoder outputs before before cross-entropy
  #                      # Possible values: {"mean", "first", "last"}, where
  #                      #                  "mean" - average along length dimension
  #                      #                  "first" - use the first position in the sequence
  #                      #                  "last" - use the last position in the sequence
  dropout_seed: 0
  weight_initialization_seed: 0
  mixed_precision: True
  boundary_casting: False
  tf_summary: False
  future_implementations: True

### Optimization
# Section 5.2: All of our models,
# including the Transformer baselines, were pretrained with the same objective, pretraining corpus, and
# up to 250k updates (although our Linformer takes much less wall-clock time to get to 250k updates,
# and was consequently trained for less time).
optimizer:
  optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
  weight_decay_rate: 0.01
  epsilon: 1e-6
  max_gradient_norm: 1.0
  disable_lr_steps_reset: True
  learning_rate:
    - steps: 25000
      scheduler: "Linear"
      initial_learning_rate: 0.0
      end_learning_rate: 0.0001
    - scheduler: "Linear"
      initial_learning_rate: 0.0001
      end_learning_rate: 0.0
      steps: 250000
  loss_scaling_factor: "dynamic"
  log_summaries: True

### Cerebras parameters
runconfig:
  max_steps: 250000 # 22.88 epochs over 11186076 WikiCorpus examples
  save_summary_steps: 100
  save_checkpoints_steps: 10000
  keep_checkpoint_max: 3
  tf_random_seed: 1202
  enable_distributed: False
  eval_steps: 1000

### CS-specific configurations
csconfig:
  use_cbfloat16: False
