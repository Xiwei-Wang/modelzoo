# Transformer Large - according to Table 3.
# Based on transformer_big(), but using transformer_base_v1() instead of transformer_base()
# https://github.com/tensorflow/tensor2tensor/blob/5623deb79cfcd28f8f8c5463b58b5bd76a81fd0d/tensor2tensor/models/transformer.py#L1981

train_input:
    data_processor : "TransformerDynamicDataProcessor"
    src_vocab_file: "./data/wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./data/transformer/wmt16_en_de/vocab.bpe.32000.de"
    src_data_dir: "./data/wmt16_en_de/pytorch/train.tok.clean.bpe.32000.en"
    tgt_data_dir: "./data/wmt16_en_de/pytorch/train.tok.clean.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    batch_size: 2048
    shuffle: True
    shuffle_seed: 1
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

eval_input:
    data_processor : "TransformerDynamicDataProcessor"
    src_vocab_file: "./data/wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./data/wmt16_en_de/vocab.bpe.32000.de"
    src_data_dir: "./data/wmt16_en_de/pytorch/newstest2014.tok.clean.bpe.32000.en"
    tgt_data_dir: "./data/wmt16_en_de/pytorch/newstest2014.tok.clean.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    shuffle: False
    shuffle_seed: 1 # also for deterministic masking
    batch_size: 1 # avoid dropping eval samples for CS runs
    # suggested batching settings for GPU (16GB) runs:
    # batch_size: 32
    # drop_last: False
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

model:
    src_vocab_size: 36550
    tgt_vocab_size: 36550

    ## Encoder
    encoder_num_hidden_layers: 6
    dropout_rate: 0.1
    relu_dropout_rate: 0.0
    use_pre_encoder_decoder_dropout: True
    use_dropout_outside_residual_path: False

    # Encoder -- Attention
    d_kv: 64 # Size of the key, query, value projections per attention head.
    num_heads: 16 # d_kv * num_heads = hidden size(i.e. d_model)
    use_projection_bias_in_attention: False

    # Position Embeddings
    position_embedding_type: "fixed"

    # Shared Weighed Embeddings
    share_embedding_weights: True
    share_encoder_decoder_embedding: True

    # T5 layer norm (no mean subtraction, and bias correction)
    use_t5_layer_norm: False
    use_pre_encoder_decoder_layer_norm: True

    # Encoder -- ffn
    d_ff: 4096 # Size of the intermediate feed forward layer in t5 blocks.
    d_model: 1024  # Size of the encoder layers and the pooler layer.
    encoder_nonlinearity: "relu" # {"gelu", "relu", "gated-gelu"}
    decoder_nonlinearity: "relu" # {"gelu", "relu", "gated-gelu"}
    layer_norm_epsilon: 1.0e-5
    use_ffn_bias: True

    ## Decoder
    decoder_num_hidden_layers: 6

    # Loss scaling weight, 1/{average_number_valid_tokens}
    lm_loss_weight: 0.033

    use_transformer_initialization: True

    # Cerebras configs.
    mixed_precision: True

optimizer:
    optimizer_type: "Adam"
    beta1: 0.9
    beta2: 0.98
    epsilon: 1.0e-6
    learning_rate:
        - scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.0004941058844013092  # (d_model(=1024)**-0.5) * (warm=4000**-0.5)
          steps: 4000
        - scheduler: "Linear"
          initial_learning_rate: 0.0004940441327439536
          end_learning_rate: 0.00022790798791340433
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 0.00022790798791340433
          end_learning_rate: 0.0001704801898581574
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 0.0001704801898581574
          end_learning_rate: 0.00014204398715647246
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 0.00014204398715647246
          end_learning_rate: 0.00012430484223196318
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 0.00012430484223196318
          end_learning_rate: 0.00011189223181306404
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 0.00011189223181306404
          end_learning_rate: 0.00010258259796518404
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 0.00010258259796518404
          end_learning_rate: 9.526686369390594e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 9.526686369390594e-05
          end_learning_rate: 8.932181488577382e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 8.932181488577382e-05
          end_learning_rate: 8.436676240964464e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 8.436676240964464e-05
          end_learning_rate: 8.015429509164194e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 8.015429509164194e-05
          end_learning_rate: 7.651572483578392e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 7.651572483578392e-05
          end_learning_rate: 7.33315566062018e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 7.33315566062018e-05
          end_learning_rate: 7.051445835878773e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 7.051445835878773e-05
          end_learning_rate: 6.799892337558537e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 6.799892337558537e-05
          end_learning_rate: 6.573472361805505e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 6.573472361805505e-05
          end_learning_rate: 6.368261354529861e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 6.368261354529861e-05
          end_learning_rate: 6.181142378334902e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 6.181142378334902e-05
          end_learning_rate: 6.009604272191723e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 6.009604272191723e-05
          end_learning_rate: 5.8515982277552135e-05
          steps: 14800
        - scheduler: "Linear"
          initial_learning_rate: 5.8515982277552135e-05
          end_learning_rate: 5.705433798297074e-05
          steps: 14800
    loss_scaling_factor: "dynamic"

runconfig:
    max_steps: 300000
    log_steps: 100
    checkpoint_steps: 10000
    seed: 1
    model_dir: "./model_dir"
    show_debug_metrics: False
    save_losses: True
    eval_steps: 3001 # comment out this line for GPU eval
