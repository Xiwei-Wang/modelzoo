# PyTorch BERT fine-tuning model for SQuAD v1.1

- [Model overview](#model-overview)
- [Sequence of the steps to perform](#Sequence-of-the-steps-to-perform)
- [Key features from CSoft platform used in this reference implementation](#Key-features-from-CSoft-platform-used-in-this-reference-implementation)
- [Code structure](#code-structure)
- [Dataset preparations](#dataset-preparations)
  - [Data download](#data-download)
  - [Data preprocessing](#Data-preprocessing)
- [Input function pipeline](#input-function-pipeline)
  - [Features labels dictionary](#Features-labels-dictionary)
- [How to run](#How-to-run)
  - [Training on GPU and CPU](#Training-on-GPU-and-CPU)
  - [Prediction on GPU and CPU](#Prediction-on-GPU-and-CPU)
  - [Evaluation on GPU and CPU](#Evaluation-on-GPU-and-CPU)
  - [How to compile and validate](#How-to-compile-and-validate)
  - [How to run training on Cerebras System](#How-to-run-training-on-Cerebras-System)
- [Configs included for this model](#Configs-included-for-this-model)

# Model overview 
This model uses the [BERT](https://arxiv.org/abs/1810.04805) architecture to solve a question answering task.

This is an extractive task in the sense that the answer is a segment of text taken verbatim from the article itself.
Here we use the Stanford Question Answering Dataset (SQuAD) consisting of questions related to a set of Wikipedia articles and the corresponding answers.
A full description of the dataset and task is available in this link [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/).


This directory contains the scripts to train the BERT model on the SQuAD v1.1 question answering (Q/A) task.
This is usually treated as a fine-tuning task, where the BERT model is initialized with weights generated by pre-training the model on a masked language modeling task (see [BERT](../../README.md)).
In this context, the goal of the model is to select the portion of the provided input context that corresponds to the answer to the question.
Accordingly, the BERT model architecture needs to be adjusted for the Q/A task being addressed here.
The masked language modeling (MLM) and next sentence prediction (NSP) output heads used in pre-training are replaced by a token classification head.
This new classification head is added to the output of the BERT encoder stack.
It head has a dense layer with two outputs. The first output predicts the location of the start
of the answer in the provided context while the second output predicts the location of the end
of the answer in the provided context. The parameters for the new output layers are initialized with random values.

Note: the TensorFlow version of this model is located at [TensorFlow SQuAD](../../../../tf/bert/fine_tuning/qa).

# Sequence of the steps to perform
The following block diagram shows a high-level view of the sequence of steps you will perform in this example:
<p align = "center">
<img src = ./images/steps-pt-qa.png>
</p>
<p align = "center">
Fig.1 - Flow Chart of steps to fine-tune Q/A model.
</p>


# Key features from CSoft platform used in this reference implementation
Fine-tuning classification model configs are supported in the [Layer Pipelined mode](https://docs.cerebras.net/en/latest/cerebras-basics/cerebras-execution-modes.html#layer-pipelined-mode).

# Code structure
* `configs/`: YAML configuration files.
* `input/`: Input pipeline implementation based on the [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/). Vocab files are located in `transformers/vocab/`.
* `model.py`: Model implementation leveraging `BertForQuestionAnsweringModel` for Q/A fine-tuning task.
* `data.py`: The entry point to the data input pipeline code.
* `run.py`: Training script. Performs training and validation.
* `utils.py`: Miscellaneous helper functions.
* `run_prediction.py`: Script to run prediction.

# Dataset preparations

## Data download

The SQuAD website no longer has links to the v1.1 data.
However, train and dev sets for SQuAD v1.1 can be downloaded by running the next commands:
```shell 
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json &&
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json
```

## Data preprocessing

Once downloaded, the raw JSON formatted data is pre-preprocessed and saved to CSV formatted files to allow for fast data streaming to the system.

To create CSV files, run the command:

```shell
python input/write_csv_qa.py \
    --do_lower_case \
    --data_dir /path/to/squad \
    --vocab_file /path/to/vocab.txt \
    --data_split_type all \
    --num_output_files 4
```

Running the script with the `--help` option will output a description of the command line arguments.

We will focus only on the arguments that are already listed in this command. The data directory `--data_dir` is the place where you stored `train-v1.1.json` file at the [Data download](#Data-download) step.
Your vocabulary file `--vocab_file` should be the same one as used for pre-training, and can be found in the appropriate pre-training config file under `train_input.vocab_file` field.  

The options `--do_lower_case`, `--max_seq_length`, `--doc_stride`, and `--max_query_length`
can be changed from their default values if necessary.

# Input function pipeline

If you want to use your own data loader with this example code, then this section describes the input data format expected by `BertForQuestionAnsweringModel` class defined in [model.py](./model.py) (the `data` variable passed to the function`__call__`).
When you create your own custom BERT Q/A input function or dataloader, you must ensure that your input function produces a features dictionary with the inputs and labels as described in this section.


The input to the model takes the format of a question followed by a context which contains the answer.
The question is preceded by the `[CLS]` special token and is separated from the context by the `[SEP]` special token.
The BERT model pre-training uses a special segment embedding layer at the input to signify the two sentences used in the NSP task.
Here, that segment embedding is used to distinguish between the question and context segments of the input (see [below for details](#features-labels-dictionary)).


The labels for this model indicate the index of the tokens from the context segment of the input that represent the answer to the question.
This is supplied to the model in the form of `2` integers specifying the start and end index of the input text to the model that comprises the answer to the question.

## Features labels dictionary

The input features and labels are passed to the model in one dictionary has the following key/values:

- `input_ids`: Input token IDs, padded with `0` to `max_sequence_length`. The tokens in the dataset are mapped to these IDs using the vocabulary file. These values should be between `0` and `vocab size - 1`, inclusive.  The first token should be the special `[CLS]`. The question and context should be separated by the special `[SEP]` token.  Also, the end of the content should be marked by another `[SEP]` token.
  Here is an example of the input:
  ```
  [CLS] <question> [SEP] <context> [SEP]
  ```
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `attention_mask`: Mask for padded positions. Has values `0` on the padded positions and `1` elsewhere. Note, this is the opposite of the masking used in the
[TensorFlow implementation](../../../../tf/bert/fine_tuning/qa) of the model.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `token_type_ids`: Segment IDs. A tensor the same size as the `input_ids` designating to which segment each token belongs. Each element of this tensor should only take the value `0` or `1`.  The `[CLS]` token at the start, the question and the subsequent `[SEP]` token should all have the segment value `0`.  The context and the subsequent `[SEP]` token should have the segment value `1`. All padding tokens after the last `[SEP]` should be in segment id `0`.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `labels`: Labels indicating the correct answer to the question. The first and second column contain the start and end index of the answer, respectively.
These indices are referring to the `input_ids` tensor supplied in the feature dictionary.
All of these indices should be greater than the index of the `[SEP]` token which designated the start of the context segment of the input.
  - Shape: `[batch_size, 2]`
  - Type: `torch.int32`

- `label_weights`: Label weighting tensor.  This is input can be used to scale the loss for each token. For standard use, this is a tensor the same size as labels and should have both values set to `1.0`.
  - Shape: `[batch_size, 2]`
    - Type: `torch.float16`

An example of the input string and segment ID structure (note the input tokens are converted to IDs using the vocab file):

```
Input Tokens:   [CLS] How old is John [SEP] John lives in California. He is 20 years old. [SEP] [PAD] [PAD] .....
Segments:   0    0   0   0   0    0    1     1    1   1          1  1  1  1     1    1      0    0   .....
Labels: (10, 14)  # location of the start and end of the answer
```

For the example above, the answer is contained in the line `He is 20 years old.`  Therefore, the labels would be the tuple `(10, 14)`

# How to run
In this section, we describe the basic commands for running training and eval. The [Running on Cerebras System](#running-on-cerebras-system) describes how to use these commands to run on Cerebras System.

## Training on GPU and CPU
To train the model on the SQuAD dataset use the following command:

```shell
python-pt run.py \
    -m train \
    --params configs/bert_base_squad.yaml \
    --checkpoint_path /path/to/pretrained/checkpoint_1000000.mdl \
    --is_pretrained_checkpoint
```

The params yaml should include the `train_input.data_dir` param where the CSV
files are saved. The list of provided param yaml files is located at [Configs included for this model ](#Configs-included-for-this-model) section.

If you want to fine-tune a model after pre-training, provide the pre-trained checkpoint weights using the ` --checkpoint_path` command and include `--is_pretrained_checkpoint` flag.
If this is omitted the model will be initialized randomly before training begins.


## Prediction on GPU and CPU

To evaluate the model on the eval dataset, first we must generate the output predictions.

After training is complete, we can form predictions on the eval set using:
```shell
python-pt run_prediction.py \
    --params configs/bert_base_squad.yaml \
    --checkpoint_path /path/to/fine_tuned/checkpoint_10000.mdl \
    --do_lower_case \
    --predict_file /path/to/dev-v1.1.json
```

where the params passed in should be the ones that were used to create the
checkpoint located at `--checkpoint_path`. The `--do_lower_case`,
`--max_seq_length`, `--doc_stride`, and `--max_query_length` need to be adjusted if
non-default values were used during the [Dataset preparations](#dataset-preparations) step described above.
To generate more than one prediction per example, `--n_best_size` can be increased.
After running this script, the `output_dir` will contain `predictions.json` and
`nbest_predictions.json` as well as generate the CSV files for evaluation.


## Evaluation on GPU and CPU

The output of the prediction script used above conforms with the output required by the official SQuAD v1.1 evaluations script.
In order to evaluate the predictions, download [evaluate-v1.1.py](https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py), the official SQuAD evaluation script, and run:

```shell
python evaluate-v1.1.py /path/to/dev-v1.1.json /path/to/predictions.json
```

which will print a dictionary with keys `exact_match` and `f1`, the evaluation metrics for this task, on the terminal.

## How to compile and validate 
**validate_only** mode runs a lightweight verification mode. The compiler will run through the first few stages of
the compilation stack up through kernel matching. This step is very fast and allows users to quickly iterate
on model code. It only runs on CPU and so can be executed without using time on a Cerebras System. It can be run using the following command:

```
csrun_cpu --mount_dirs=/path1,/path2 python-pt run.py \
    -m train \
    --params configs/bert_base_squad.yaml \
    --checkpoint_path /path/to/pretrained/checkpoint_1000000.mdl \
    --is_pretrained_checkpoint \
    --validate_only
```

**compile_only** mode executes a full model compilation on CPU to generate a CS system executable.
It will not run this executable on CS system in this mode, but when `--compile_only` mode is successful,
your model is likely to run on CS system. It can be run using the command:

```
csrun_cpu --mount_dirs=/path1,/path2 python-pt run.py \
    -m train \
    --params configs/bert_base_squad.yaml \
    --checkpoint_path /path/to/pretrained/checkpoint_1000000.mdl \
    --is_pretrained_checkpoint \
    --compile_only
```

You can then run the generated executable on Cerebras System by executing a `csrun_wse` command (see above) specifying the
same `model_dir` as used for the compile command.

## How to run training on Cerebras System
To run training in [pipeline mode](https://docs.cerebras.net/en/latest/cerebras-basics/cerebras-execution-modes.html#layer-pipelined-mode) on the Cerebras System, you will need to modify the above training command to run inside of the Cerebras environment. In addition, the `cs_ip` should be provided either as a command line argument `--cs_ip` or in the YAML config file.

Follow [How to train on the CS System](../../../../../#how-to-train-on-the-cs-system) and execute the following from within the Cerebras environment:

```
csrun_wse python-pt run.py \
    -m train \
    --params configs/bert_base_squad.yaml \
    --checkpoint_path /path/to/pretrained/checkpoint_1000000.mdl \
    --is_pretrained_checkpoint \
    --cs_ip x.x.x.x
```

# Configs included for this model 
In order to train the model, you need to provide a yaml config file. Some popular yaml config files are listed below for reference. 
Also, feel free to create your own following these examples:  

* [bert_base_squad.yaml](configs/bert_base_squad.yaml) has a standard bert-base config with `hidden_size=768`, `num_hidden_layers=12`, `num_heads=12`.
* [bert_large_squad.yaml](configs/bert_large_squad.yaml) has a standard bert-large config with `hidden_size=1024`, `num_hidden_layers=24`, `num_heads=16`.
