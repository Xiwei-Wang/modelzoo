# Config from gptj/configs/params_pretraining.yaml
# with CNN Daily Mail dataset, bsz 32, and MSL 1024

train_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir: "./cnn_dailymail/train_msl1024/"
  vocab_size: 50400
  max_sequence_length: 1024
  shuffle: True
  repeat: True
  batch_size: 32

eval_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir: "./cnn_dailymail/val_msl1024/"
  vocab_size: 50400
  max_sequence_length: 1024
  shuffle: True
  repeat: False
  batch_size: 32

model:
  embedding_dropout_rate: 0.1
  share_embedding_weights: True

  max_position_embeddings: 1024
  use_position_embeddings: False

  hidden_size: 4096
  num_heads: 16
  num_hidden_layers: 28

  use_projection_bias_in_attention: False
  use_ffn_bias_in_attention: False
  use_ffn_bias: True

  filter_size: 16384
  nonlinearity: "gelu"
  attention_dropout_rate: 0.1
  residual_dropout_rate: 0.1

  rotary_dim: 64
  layer_norm_epsilon: 1.0e-5
  use_cache: False
  use_bias_in_output: True

  embedding_initializer:
    - name: "scaled_init_normal"
      key: "vocab_size"

  initializer:
    - name: "variance_scaling"
      scale: 1.0

  output_layer_initializer:
    - name: "variance_scaling"
      scale_type: "wang_init"

  mixed_precision: True
  boundary_casting: False
  tf_summary: False

optimizer:
  optimizer_type: "adamw"
  epsilon: 1.0e-8
  weight_decay_rate: 0.1
  max_gradient_norm: 1.0
  use_bias_correction: True
  max_loss_scale: 4290774016.0
  learning_rate:
  - end_learning_rate: 5.0e-05
    initial_learning_rate: 0.0
    scheduler: Linear
    steps: 200
  - end_learning_rate: 0
    initial_learning_rate: 5.0e-05
    scheduler: Linear
    steps: 300
  loss_scaling_factor: "dynamic"

runconfig:
  max_steps: 500
  save_summary_steps: 50
  save_checkpoints_steps: 100
  keep_checkpoint_max: 0
  enable_distributed: False
