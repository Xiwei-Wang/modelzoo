# GPT-J 6B model

train_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir: "./input/pile_pretraining_gpt/train_msl2048/"
  vocab_size: 50400
  max_sequence_length: 2048
  shuffle: True
  repeat: True
  batch_size: 30

eval_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir: "./input/pile_pretraining_gpt/val_msl2048/"
  vocab_size: 50400
  max_sequence_length: 2048
  shuffle: True
  repeat: False
  batch_size: 30

model:
  embedding_size: 4096
  embedding_dropout_rate: 0.1
  share_embedding_weights: True

  hidden_size: 4096
  num_heads: 16
  num_hidden_layers: 28
  max_position_embeddings: 2048

  use_projection_bias_in_attention: False
  use_ffn_bias_in_attention: False
  use_ffn_bias: True

  filter_size: 16384
  nonlinearity: "gelu"
  attention_dropout_rate: 0.1
  residual_dropout_rate: 0.1

  rotary_dim: 64
  layer_norm_epsilon: 1.0e-5
  use_cache: False
  use_bias_in_output: True

  embedding_initializer:
    - name: "scaled_init_normal"
      key: "vocab_size"

  initializer:
    - name: "variance_scaling"
      scale: 1.0

  output_layer_initializer:
    - name: "variance_scaling"
      scale_type: "wang_init"

  mixed_precision: True
  boundary_casting: False
  tf_summary: False

optimizer:
  optimizer_type: "adamw"
  epsilon: 1.0e-6
  weight_decay_rate: 0.1
  max_gradient_norm: 1.0
  learning_rate:
    - steps: 300
      scheduler: "Linear"
      initial_learning_rate: 0.0
      end_learning_rate: 1.5e-5
    - scheduler: "Linear"
      initial_learning_rate: 1.5e-5
      end_learning_rate: 1.5e-6
      steps: 30000
    - scheduler: "Constant"
      learning_rate: 1.5e-6
  loss_scaling_factor: "dynamic"

runconfig:
  max_steps: 150000
  save_summary_steps: 500
  save_checkpoints_steps: 5000
  keep_checkpoint_max: 5
  enable_distributed: False
